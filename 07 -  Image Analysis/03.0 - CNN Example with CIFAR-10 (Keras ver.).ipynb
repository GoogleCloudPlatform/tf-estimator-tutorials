{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Download CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "\n",
    "def maybe_download_and_extract(\n",
    "  dest_directory='data',\n",
    "  data_url='http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'):\n",
    "  \n",
    "  \"\"\"Download and extract the tarball from Alex's website.\"\"\"\n",
    "  dest_directory = dest_directory\n",
    "  if not os.path.exists(dest_directory):\n",
    "    os.makedirs(dest_directory)\n",
    "  filename = data_url.split('/')[-1]\n",
    "  filepath = os.path.join(dest_directory, filename)\n",
    "  if not os.path.exists(filepath):\n",
    "    def _progress(count, block_size, total_size):\n",
    "      sys.stdout.write('\\r>> Downloading %s %.1f%%' % (\n",
    "          filename, float(count * block_size) / float(total_size) * 100.0))\n",
    "      sys.stdout.flush()\n",
    "    filepath, _ = urllib.request.urlretrieve(DATA_URL, filepath, _progress)\n",
    "    print()\n",
    "    statinfo = os.stat(filepath)\n",
    "    print('Successfully downloaded', filename, statinfo.st_size, 'bytes.')\n",
    "  extracted_dir_path = os.path.join(dest_directory, 'cifar-10-batches-bin')\n",
    "  if not os.path.exists(extracted_dir_path):\n",
    "    tarfile.open(filepath, 'r:gz').extractall(dest_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = 'data'\n",
    "DATA_URL = 'http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'\n",
    "maybe_download_and_extract(DATA_DIR, DATA_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Exploring CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extract_data(index=0, filepath='data/cifar-10-batches-bin/data_batch_5.bin'):\n",
    "  bytestream = open(filepath, mode='rb')\n",
    "\n",
    "  label_bytes_length = 1\n",
    "  image_bytes_length = (32 ** 2) * 3\n",
    "  record_bytes_length = label_bytes_length + image_bytes_length\n",
    "\n",
    "  bytestream.seek(record_bytes_length * index, 0)\n",
    "  label_bytes = bytestream.read(label_bytes_length)\n",
    "  image_bytes = bytestream.read(image_bytes_length)\n",
    "\n",
    "  label = np.frombuffer(label_bytes, dtype=np.uint8)  \n",
    "  image = np.frombuffer(image_bytes, dtype=np.uint8)\n",
    "  \n",
    "  image = np.reshape(image, [3, 32, 32])\n",
    "  image = np.transpose(image, [1, 2, 0])\n",
    "  image = image.astype(np.float32)\n",
    "  \n",
    "  result = {\n",
    "    'image': image,\n",
    "    'label': label,\n",
    "  }\n",
    "  bytestream.close()\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f29221f2b90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUeT8QiSKnF+IRJHzC5Eocn4hEuX/Acln+nPxL0f4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f295c58d610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "result = extract_data(np.random.randint(1000), filepath='data/cifar-10-batches-bin/test_batch.bin')\n",
    "plt.imshow(result['image'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to use the TF Estimator APIs\n",
    "1. Define dataset **metadata** and **global constants**\n",
    "2. Define **data input function** to read the data from the source + **apply pre-processing**\n",
    "3. Create TF **feature columns** based on metadata + **extended feature columns**\n",
    "4. Instantiate a **model function** with the required **feature columns, EstimatorSpecs, & parameters**\n",
    "5. Define a **serving function**\n",
    "6. Run **Experiment** by supplying training and validation data, as well as required parameters\n",
    "7. **Evaluate** the model using test data\n",
    "8. Perform **predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "from tensorflow.python.feature_column import feature_column\n",
    "\n",
    "from tensorflow.contrib.learn import learn_runner\n",
    "from tensorflow.contrib.learn import make_export_strategy\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_files = ['data/cifar-10-batches-bin/data_batch_{}.bin'.format(i) for i in range(1,5)]\n",
    "valid_data_files = ['data/cifar-10-batches-bin/data_batch_5.bin']\n",
    "test_data_files = ['data/cifar-10-batches-bin/test_batch.bin']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define dataset metadata and global constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Process images of this size. Note that this differs from the original CIFAR\n",
    "# image size of 32 x 32. If one alters this number, then the entire model\n",
    "# architecture will change and any model would need to be retrained.\n",
    "IMAGE_HEIGHT = 32\n",
    "IMAGE_WIDTH = 32\n",
    "IMAGE_DEPTH = 3\n",
    "\n",
    "# Global constants describing the CIFAR-10 data set.\n",
    "NUM_CLASSES = 10\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = 50000\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = 10000\n",
    "\n",
    "# If a model is trained with multiple GPUs, prefix all Op names with tower_name\n",
    "# to differentiate the operations. Note that this prefix is removed from the\n",
    "# names of the summaries when visualizing a model.\n",
    "TOWER_NAME = 'tower'\n",
    "\n",
    "# We use a weight decay of 0.0002, which performs better than the 0.0001 that\n",
    "# was originally suggested.\n",
    "WEIGHT_DECAY = 2e-4\n",
    "MOMENTUM = 0.9\n",
    "\n",
    "# Global constants describing model behaviors\n",
    "MODEL_NAME = 'cnn-model-03'\n",
    "USE_CHECKPOINT = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Data Input Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. parsing CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_record(raw_record):\n",
    "  # Every record consists of a label followed by the image, with a fixed number\n",
    "  # of bytes for each.\n",
    "  label_bytes = 1\n",
    "  image_bytes = IMAGE_HEIGHT * IMAGE_WIDTH * IMAGE_DEPTH\n",
    "  record_bytes = label_bytes + image_bytes\n",
    "  \n",
    "  # Convert from a string to a vector of uint8 that is record_bytes long.\n",
    "  record_vector = tf.decode_raw(raw_record, tf.uint8)\n",
    "  \n",
    "  # The first byte represents the label, which we convert from uint8 to int32\n",
    "  # and then to one-hot.\n",
    "  label = tf.cast(record_vector[0], tf.int32)\n",
    "  label = tf.one_hot(label, NUM_CLASSES)\n",
    "  \n",
    "  # The remaining bytes after the label represent the image, which we reshape\n",
    "  # from [depth * height * width] to [depth, height, width].\n",
    "  depth_major = tf.reshape(\n",
    "    record_vector[label_bytes:record_bytes], [IMAGE_DEPTH, IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "  \n",
    "  # Convert from [depth, height, width] to [height, width, depth], and cast as\n",
    "  # float32.\n",
    "  image = tf.cast(tf.transpose(depth_major, [1, 2, 0]), tf.float32)\n",
    "  \n",
    "  return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. preprocessing CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image, is_training=False):\n",
    "  \"\"\"Preprocess a single image of layout [height, width, depth].\"\"\"\n",
    "  if is_training:\n",
    "    # Resize the image to add four extra pixels on each side.\n",
    "    image = tf.image.resize_image_with_crop_or_pad(\n",
    "        image, IMAGE_HEIGHT + 8, IMAGE_WIDTH + 8)\n",
    "\n",
    "    # Randomly crop a [_HEIGHT, _WIDTH] section of the image.\n",
    "    image = tf.random_crop(image, [IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH])\n",
    "\n",
    "    # Randomly flip the image horizontally.\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "\n",
    "  # Subtract off the mean and divide by the variance of the pixels.\n",
    "  image = tf.image.per_image_standardization(image)\n",
    "  return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. data pipeline input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_input_fn(file_names,\n",
    "                      mode=tf.estimator.ModeKeys.EVAL,\n",
    "                      num_epochs=None,\n",
    "                      batch_size=1):\n",
    "\n",
    "  def _input_fn():\n",
    "    label_bytes = 1\n",
    "    image_bytes = IMAGE_HEIGHT * IMAGE_WIDTH * IMAGE_DEPTH\n",
    "    record_bytes = label_bytes + image_bytes\n",
    "    dataset = tf.data.FixedLengthRecordDataset(filenames=file_names,\n",
    "                                               record_bytes=record_bytes)\n",
    "\n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    if is_training:\n",
    "      buffer_size = batch_size * 2 + 1\n",
    "      dataset = dataset.shuffle(buffer_size=buffer_size)\n",
    "\n",
    "    dataset = dataset.map(parse_record)\n",
    "    dataset = dataset.map(lambda image, label: (preprocess_image(image, is_training), label))\n",
    "\n",
    "    dataset = dataset.prefetch(2 * batch_size)\n",
    "\n",
    "    # We call repeat after shuffling, rather than before, to prevent separate\n",
    "    # epochs from blending together.\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "\n",
    "    # Batch results by up to batch_size, and then fetch the tuple from the\n",
    "    # iterator.\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    images, labels = iterator.get_next()\n",
    "\n",
    "    features = {'images': images}\n",
    "    return features, labels\n",
    "  \n",
    "  return _input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Feature Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feature_columns():\n",
    "  feature_columns = {\n",
    "    'images': tf.feature_column.numeric_column('images', (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH)),\n",
    "  }\n",
    "  return feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Columns: {'images': _NumericColumn(key='images', shape=(32, 32, 3), default_value=None, dtype=tf.float32, normalizer_fn=None)}\n"
     ]
    }
   ],
   "source": [
    "feature_columns = get_feature_columns()\n",
    "print(\"Feature Columns: {}\".format(feature_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Instantiate an Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "  model = tf.keras.models.Sequential()\n",
    "  # Define input tensor in Keras world.\n",
    "  model.add(tf.keras.layers.InputLayer(\n",
    "    input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_DEPTH), name='images'))\n",
    "\n",
    "  # The first convolutional layer.\n",
    "  model.add(tf.keras.layers.Conv2D(\n",
    "    filters=32, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "  model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='same'))\n",
    "\n",
    "  # The second convolutional layer.\n",
    "  model.add(tf.keras.layers.Conv2D(\n",
    "    filters=32, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "  model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='same'))\n",
    "  model.add(tf.keras.layers.Dropout(0.25))\n",
    "    \n",
    "  # The third convolutional layer\n",
    "  model.add(tf.keras.layers.Conv2D(\n",
    "    filters=64, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    \n",
    "  # The fourth convolutional layer\n",
    "  model.add(tf.keras.layers.Conv2D(\n",
    "    filters=64, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "  model.add(tf.keras.layers.Dropout(0.25))\n",
    "    \n",
    "  model.add(tf.keras.layers.Flatten())\n",
    "    \n",
    "  model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "  model.add(tf.keras.layers.Dense(NUM_CLASSES))\n",
    "  model.add(tf.keras.layers.Activation('softmax'))\n",
    "  \n",
    "  opt = tf.keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
    "  model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train, Evaluate and Export ML Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Set HParam and RunConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hparams = tf.contrib.training.HParams(\n",
    "  batch_size=200,\n",
    "  max_steps=100,\n",
    ")\n",
    "\n",
    "model_dir = 'trained_models/{}'.format(MODEL_NAME)\n",
    "\n",
    "run_config = tf.contrib.learn.RunConfig(\n",
    "  save_checkpoints_steps=200,\n",
    "  tf_random_seed=19851211,\n",
    "  model_dir=model_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Define Serving Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cannot use this\n",
    "def serving_input_fn():\n",
    "\n",
    "  receiver_tensor = {'images': tf.placeholder(shape=[None, 32, 32, 3], dtype=tf.float32)}\n",
    "  features = {'images': tf.map_fn(preprocess_image, receiver_tensor['images'])}\n",
    "  \n",
    "  return tf.estimator.export.ServingInputReceiver(features, receiver_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using the Keras model from memory.\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f29067bebd0>, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': 'trained_models/cnn-model-03', '_save_summary_steps': 100}\n"
     ]
    }
   ],
   "source": [
    "# Set learning phase as Training\n",
    "tf.keras.backend.set_learning_phase(True)\n",
    "\n",
    "# Get model defined with tf.keras\n",
    "keras_model = get_model()\n",
    "\n",
    "# Create estimator from keras model\n",
    "estimator = tf.keras.estimator.model_to_estimator(\n",
    "  keras_model=keras_model, model_dir=model_dir)\n",
    "\n",
    "# TF.Keras seems not to support exporter...\n",
    "exporter = None\n",
    "\n",
    "train_spec = tf.estimator.TrainSpec(\n",
    "  input_fn=generate_input_fn(file_names=train_data_files,\n",
    "                             mode=tf.contrib.learn.ModeKeys.TRAIN,\n",
    "                             batch_size=hparams.batch_size),\n",
    "  max_steps=hparams.max_steps,\n",
    "  hooks=None\n",
    ")\n",
    "\n",
    "eval_spec = tf.estimator.EvalSpec(\n",
    "  input_fn=generate_input_fn(file_names=valid_data_files,\n",
    "                             mode=tf.contrib.learn.ModeKeys.EVAL,\n",
    "                             batch_size=hparams.batch_size),\n",
    "  steps=50,\n",
    "  name=None,\n",
    "  hooks=None,\n",
    "  exporters=exporter, # Iterable of Exporters, or single one or None.\n",
    "  start_delay_secs=120,\n",
    "  throttle_secs=600\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 600 secs (eval_spec.throttle_secs) or training is finished.\n",
      "INFO:tensorflow:Skipping training since max_steps has already saved.\n",
      "INFO:tensorflow:Starting evaluation at 2017-11-30-03:33:13\n",
      "INFO:tensorflow:Restoring parameters from trained_models/cnn-model-03/model.ckpt-100\n",
      "INFO:tensorflow:Evaluation [1/50]\n",
      "INFO:tensorflow:Evaluation [2/50]\n",
      "INFO:tensorflow:Evaluation [3/50]\n",
      "INFO:tensorflow:Evaluation [4/50]\n",
      "INFO:tensorflow:Evaluation [5/50]\n",
      "INFO:tensorflow:Evaluation [6/50]\n",
      "INFO:tensorflow:Evaluation [7/50]\n",
      "INFO:tensorflow:Evaluation [8/50]\n",
      "INFO:tensorflow:Evaluation [9/50]\n",
      "INFO:tensorflow:Evaluation [10/50]\n",
      "INFO:tensorflow:Evaluation [11/50]\n",
      "INFO:tensorflow:Evaluation [12/50]\n",
      "INFO:tensorflow:Evaluation [13/50]\n",
      "INFO:tensorflow:Evaluation [14/50]\n",
      "INFO:tensorflow:Evaluation [15/50]\n",
      "INFO:tensorflow:Evaluation [16/50]\n",
      "INFO:tensorflow:Evaluation [17/50]\n",
      "INFO:tensorflow:Evaluation [18/50]\n",
      "INFO:tensorflow:Evaluation [19/50]\n",
      "INFO:tensorflow:Evaluation [20/50]\n",
      "INFO:tensorflow:Evaluation [21/50]\n",
      "INFO:tensorflow:Evaluation [22/50]\n",
      "INFO:tensorflow:Evaluation [23/50]\n",
      "INFO:tensorflow:Evaluation [24/50]\n",
      "INFO:tensorflow:Evaluation [25/50]\n",
      "INFO:tensorflow:Evaluation [26/50]\n",
      "INFO:tensorflow:Evaluation [27/50]\n",
      "INFO:tensorflow:Evaluation [28/50]\n",
      "INFO:tensorflow:Evaluation [29/50]\n",
      "INFO:tensorflow:Evaluation [30/50]\n",
      "INFO:tensorflow:Evaluation [31/50]\n",
      "INFO:tensorflow:Evaluation [32/50]\n",
      "INFO:tensorflow:Evaluation [33/50]\n",
      "INFO:tensorflow:Evaluation [34/50]\n",
      "INFO:tensorflow:Evaluation [35/50]\n",
      "INFO:tensorflow:Evaluation [36/50]\n",
      "INFO:tensorflow:Evaluation [37/50]\n",
      "INFO:tensorflow:Evaluation [38/50]\n",
      "INFO:tensorflow:Evaluation [39/50]\n",
      "INFO:tensorflow:Evaluation [40/50]\n",
      "INFO:tensorflow:Evaluation [41/50]\n",
      "INFO:tensorflow:Evaluation [42/50]\n",
      "INFO:tensorflow:Evaluation [43/50]\n",
      "INFO:tensorflow:Evaluation [44/50]\n",
      "INFO:tensorflow:Evaluation [45/50]\n",
      "INFO:tensorflow:Evaluation [46/50]\n",
      "INFO:tensorflow:Evaluation [47/50]\n",
      "INFO:tensorflow:Evaluation [48/50]\n",
      "INFO:tensorflow:Evaluation [49/50]\n",
      "INFO:tensorflow:Evaluation [50/50]\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-30-03:33:17\n",
      "INFO:tensorflow:Saving dict for global step 100: accuracy = 0.3136, global_step = 100, loss = 1.88934\n"
     ]
    }
   ],
   "source": [
    "#if not USE_CHECKPOINT:\n",
    "#  print(\"Removing previous artifacts...\")\n",
    "#  shutil.rmtree(model_dir, ignore_errors=True)\n",
    "\n",
    "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2017-11-30-03:33:17\n",
      "INFO:tensorflow:Restoring parameters from trained_models/cnn-model-03/model.ckpt-100\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-30-03:33:18\n",
      "INFO:tensorflow:Saving dict for global step 100: accuracy = 0.29, global_step = 100, loss = 1.95585\n",
      "######################################################################################\n",
      "# {'loss': 1.9558514, 'global_step': 100, 'accuracy': 0.28999999}\n",
      "######################################################################################\n",
      "INFO:tensorflow:Starting evaluation at 2017-11-30-03:33:18\n",
      "INFO:tensorflow:Restoring parameters from trained_models/cnn-model-03/model.ckpt-100\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-30-03:33:19\n",
      "INFO:tensorflow:Saving dict for global step 100: accuracy = 0.311, global_step = 100, loss = 1.90885\n",
      "######################################################################################\n",
      "# {'loss': 1.9088508, 'global_step': 100, 'accuracy': 0.31099999}\n",
      "######################################################################################\n",
      "INFO:tensorflow:Starting evaluation at 2017-11-30-03:33:19\n",
      "INFO:tensorflow:Restoring parameters from trained_models/cnn-model-03/model.ckpt-100\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-30-03:33:20\n",
      "INFO:tensorflow:Saving dict for global step 100: accuracy = 0.344, global_step = 100, loss = 1.88284\n",
      "######################################################################################\n",
      "# {'loss': 1.8828421, 'global_step': 100, 'accuracy': 0.34400001}\n",
      "######################################################################################\n"
     ]
    }
   ],
   "source": [
    "train_size = 1000\n",
    "valid_size = 1000\n",
    "test_size = 1000\n",
    "\n",
    "train_input_fn = generate_input_fn(file_names=train_data_files,\n",
    "                                   mode=tf.contrib.learn.ModeKeys.TRAIN,\n",
    "                                   batch_size=train_size)\n",
    "\n",
    "valid_input_fn = generate_input_fn(file_names=valid_data_files,\n",
    "                                   mode=tf.contrib.learn.ModeKeys.EVAL,\n",
    "                                   batch_size=valid_size)\n",
    "\n",
    "test_input_fn = generate_input_fn(file_names=test_data_files,\n",
    "                                  mode=tf.contrib.learn.ModeKeys.EVAL,\n",
    "                                  batch_size=test_size)\n",
    "\n",
    "train_results = estimator.evaluate(input_fn=train_input_fn, steps=1)\n",
    "print(\"######################################################################################\")\n",
    "print(\"# {}\".format(train_results))\n",
    "print(\"######################################################################################\")\n",
    "\n",
    "valid_results = estimator.evaluate(input_fn=valid_input_fn, steps=1)\n",
    "print(\"######################################################################################\")\n",
    "print(\"# {}\".format(valid_results))\n",
    "print(\"######################################################################################\")\n",
    "\n",
    "test_results = estimator.evaluate(input_fn=test_input_fn, steps=1)\n",
    "print(\"######################################################################################\")\n",
    "print(\"# {}\".format(test_results))\n",
    "print(\"######################################################################################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that this part doesn't work as intended, so please ignore this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing previously exported model...\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: trained_models/cnn-model-03/export/Servo/1/saved_model.pb\n",
      "INFO:tensorflow:Restoring parameters from trained_models/cnn-model-03/export/Servo/1/variables/variables\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "# This cell doesn't work as intended.\n",
    "####\n",
    "\n",
    "export_dir = model_dir + '/export/Servo/'\n",
    "saved_model_dir = export_dir + '1'\n",
    "\n",
    "print(\"Removing previously exported model...\")\n",
    "shutil.rmtree(saved_model_dir, ignore_errors=True)\n",
    "\n",
    "test_type = 0\n",
    "\n",
    "if test_type == 0:\n",
    "  \"\"\"\n",
    "  This way doesn't reflect tf.keras.backend.set_learning_phase(False)\n",
    "  \"\"\"\n",
    "\n",
    "  tf.keras.backend.set_learning_phase(0)\n",
    "\n",
    "  builder = tf.saved_model.builder.SavedModelBuilder(saved_model_dir)\n",
    "\n",
    "  prediction_signature = tf.saved_model.signature_def_utils.predict_signature_def(\n",
    "    inputs={'images': keras_model.input},\n",
    "    outputs={'prediction': keras_model.output})\n",
    "\n",
    "  with tf.keras.backend.get_session() as sess:\n",
    "    builder.add_meta_graph_and_variables(\n",
    "      sess, [tf.saved_model.tag_constants.SERVING],\n",
    "      signature_def_map={\n",
    "        tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:\n",
    "          prediction_signature,\n",
    "      })\n",
    "    builder.save()\n",
    "elif test_type == 1:\n",
    "  \"\"\"\n",
    "  This way reflects tf.keras.backend.set_learning_phase(False)\n",
    "  \"\"\"\n",
    "\n",
    "  tf.keras.backend.set_learning_phase(False)\n",
    "\n",
    "  config = keras_model.get_config()\n",
    "  weights = keras_model.get_weights()\n",
    "  export_model = tf.keras.models.Sequential.from_config(config)\n",
    "  export_model.set_weights(weights)\n",
    "\n",
    "  builder = tf.saved_model.builder.SavedModelBuilder(saved_model_dir)\n",
    "\n",
    "  prediction_signature = tf.saved_model.signature_def_utils.predict_signature_def(\n",
    "    inputs={'images': export_model.input},\n",
    "    outputs={'prediction': export_model.output})\n",
    "\n",
    "  with tf.keras.backend.get_session() as sess:\n",
    "    builder.add_meta_graph_and_variables(\n",
    "      sess, [tf.saved_model.tag_constants.SERVING],\n",
    "      signature_def_map={\n",
    "        tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:\n",
    "          prediction_signature,\n",
    "      })\n",
    "    builder.save()\n",
    "\n",
    "# Create predictor function\n",
    "predictor_fn = tf.contrib.predictor.from_saved_model(\n",
    "  export_dir = saved_model_dir,\n",
    "  signature_def_key=tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def per_image_standardization(image):\n",
    "  \"\"\"\n",
    "  This is copy of tf.image.per_image_standardization\n",
    "  \"\"\"\n",
    "  image_mean = np.mean(image)\n",
    "  num_pixels = image.size\n",
    "  variance = (np.mean(image ** 2) - (image_mean ** 2))\n",
    "  variance = np.maximum(variance, 0)\n",
    "  stddev = np.sqrt(variance)\n",
    "  min_stddev = 1.0/np.sqrt(num_pixels)\n",
    "\n",
    "  pixel_value_scale = np.max([stddev, min_stddev])\n",
    "  pixel_value_offset = image_mean\n",
    "  image = image - pixel_value_offset\n",
    "  image = image / pixel_value_scale\n",
    "  return image\n",
    "\n",
    "def get_accuracy_via_predictor_function(predictor_fn,\n",
    "                                        N=1000,\n",
    "                                        filepath='data/cifar-10-batches-bin/test_batch.bin'):\n",
    "  labels = []\n",
    "  images = []\n",
    "\n",
    "  for i in range(N):\n",
    "    result = extract_data(i, filepath='data/cifar-10-batches-bin/test_batch.bin')\n",
    "    images.append(per_image_standardization(result['image']))\n",
    "    labels.append(result['label'][0])\n",
    "\n",
    "  output = predictor_fn(\n",
    "    {\n",
    "      'images': images,\n",
    "    }\n",
    "  )\n",
    "  \n",
    "  return output['prediction'], np.sum([a==r for a, r in zip(labels, np.argmax(output['prediction'], axis=1))]) / float(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.107\n"
     ]
    }
   ],
   "source": [
    "# OMG, accuracy is just the same as random predictions....\n",
    "# This might be caused by initialized weights. but how we can properly load learned weights?\n",
    "outputs_, accuracy = get_accuracy_via_predictor_function(predictor_fn=predictor_fn, N=1000)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Input graph does not contain a QueueRunner. That means predict yields forever. This is probably a mistake.\n",
      "INFO:tensorflow:Restoring parameters from trained_models/cnn-model-03/model.ckpt-100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.34399999999999997"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = []\n",
    "N = 1000\n",
    "\n",
    "test_results = estimator.predict(input_fn=test_input_fn)\n",
    "for i, output in enumerate(test_results):\n",
    "  if i == N:\n",
    "    break\n",
    "  outputs.append(output['activation_1'])\n",
    "  \n",
    "labels = []\n",
    "for i in range(N):\n",
    "  result = extract_data(i, filepath='data/cifar-10-batches-bin/test_batch.bin')\n",
    "  labels.append(result['label'][0])\n",
    "  \n",
    "np.sum([a==r for a, r in zip(labels, np.argmax(outputs, axis=1))]) / float(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image = extract_data(0)['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "A = tf.image.per_image_standardization(tf.constant(image)).eval()\n",
    "B = per_image_standardization(image)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.3876706e-07"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(A - B).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from google.datalab.ml import TensorBoard\n",
    "TensorBoard().start(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for pid in TensorBoard.list()['pid']:\n",
    "    TensorBoard().stop(pid)\n",
    "    print 'Stopped TensorBoard with pid {}'.format(pid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Using gRPC to get prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from grpc.beta import implementations\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow_serving.apis import predict_pb2\n",
    "from tensorflow_serving.apis import prediction_service_pb2\n",
    "\n",
    "def predict_with_grpc():\n",
    "  \"\"\"                                                                                                                                     \n",
    "  Note that you are running TensorFlow Serving with below commands.                                                                       \n",
    "  tensorflow_model_server --port=9000 --model_name=cnn-model-01 --model_base_path=./cnn-model-01                                          \n",
    "\n",
    "  In addition, make sure cnn-model-01 directory is organized as follows:                                                                  \n",
    "\n",
    "  cnn-model-01/:                                                                                                                          \n",
    "  {random_value}                                                                                                                              \n",
    "\n",
    "  cnn-model-01/{randam_value}/:                                                                                                           \n",
    "  saved_model.pb  variables                                                                                                               \n",
    "  \"\"\"\n",
    "  host = 'localhost'\n",
    "  port = '9000'\n",
    "  channel = implementations.insecure_channel(host, int(port))\n",
    "  stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\n",
    "\n",
    "  result = extract_data(0)\n",
    "  request = predict_pb2.PredictRequest()\n",
    "  request.model_spec.name = 'cnn-model-01'\n",
    "  request.model_spec.signature_name = 'predictions'\n",
    "  image = result['image']\n",
    "  label = result['label']\n",
    "  request.inputs['images'].CopyFrom(\n",
    "      tf.contrib.util.make_tensor_proto(image, shape=[1, 32, 32, 3]))\n",
    "\n",
    "  result_future = stub.Predict.future(request, 5.0)\n",
    "  print(result_future.result().outputs['classes'].int64_val)\n",
    "  print(result_future.result().outputs['probabilities'].float_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
